%!TEX root = ./main.tex

\setcounter{section}{4}
\setcounter{equation}{56}
\newcommand{\likpri}[1]{p(\vx\given\cC_{#1})p(\cC_{#1})}

\section*{4.2 確率的生成モデル}


\begin{align}
p(\cC_1\given\vx) & = \frac{\likpri{1}}{\likpri{1} + \likpri{2}} \notag \\
& = \frac{1}{1 + \exp(-a)} \notag \\
& = \sigma(a)
\end{align}
ここで $a$ を
\begin{equation}
  a = \ln \frac{\likpri{1}}{\likpri{2}}
\end{equation}
と定義した．

\paragraph{シグモイド関数}
$\sigma(\cdot)$ は\emph{シグモイド関数 (logistic sigmoid function)}
\begin{equation}
\sigma(z) = \frac{1}{1 + \exp(-z)}
\end{equation}

\begin{itemize}
  \item 入力 $z$ を区間 $[0, 1]$ に押し込んで出力するので，なんらかの計算を行った最後で確率を出力したい時によく出てくる．(e.g. ロジスティック回帰，ニューラルネットワーク)
  \item 対称性がある
  \begin{equation}
    \sigma(-z) = 1 - \sigma(z)
  \end{equation}
  \item 逆関数は \emph{ロジット関数 (logit function)} として知られている（統計学の世界ではリンク関数，i.e. 活性化関数の逆関数, を考えることが多い）．
  \begin{equation}
    z = \mathrm{logit}(\sigma) = \ln \parens{\frac{\sigma}{1-\sigma}}
  \end{equation}
\end{itemize}

\newpage
ロジスティック関数を他クラスに拡張したものは \emph{ソフトマックス関数 (softmax function)} として知られている．
\begin{align*}
  \vz \in \R^M,\, \vs(\vz) & = \left(\begin{array}{c}
    s_1(\vz) \\
    \vdots \\
    s_{M}(\vz)
    \end{array}\right) \\
  s_i(\vz) & = \frac{\exp(\vz_i)}{\sum_j \exp(\vz_j)}
\end{align*}

\begin{equation}
  p(\cC_k \given \vx) = \frac{\likpri{k}}{\sum_j \likpri{j}} = s_k(\ln \likpri{k})
\end{equation}


\subsection*{4.2.1 連続値入力}

クラスの条件付き確率 $p(\vx \given \cC_k)$ がガウス分布であると仮定して，事後確率がどうなるかを考える．
すべてのクラスが同じ共分散行列を共有していると仮定する．
\[
p(\vx \given \cC_k) = \parens{2\pi}^{-\frac{D}{2}} \abs{\mSigma}^{-\frac{1}{2}} \exp \braces{-\frac{1}{2} (\vx - \vmu_k)^\top \mSigma^{-1} (\vx - \vmu_k) } \tag{4.64}
\]

\subsubsection*{2クラス}

\begin{equation}
  p(\cC_1 \given \vx) = \sigma(\vx^\top \vx + w_0)
\end{equation}
\begin{align}
  \vw & = \mSigma^{-1} (\vmu_1 - \vmu_2) \\
  w_0 & = -\frac{1}{2} \vmu_1^\top \mSigma^{-1} \vmu_1 + \frac{1}{2} \vmu_2^\top \mSigma^{-1} \vmu_2 + \ln \frac{p(\cC_1)}{p(\cC_2)}
\end{align}

\begin{itemize}
  \item シグモイド関数の中身が $\vx$ の線形関数である（i.e. 境界面 $\sigma(\cdot) = 0.5 \iff \vx^\top \vx + w_0 = 0$ が 入力空間において線形）．
  \item 事前分布バイアスパラメータ $w_0$ にしか影響しない（i.e. 境界面を平行移動させるだけ）．
\end{itemize}

\subsubsection*{Kクラス}

\begin{align}
  p(\cC_k \given \vx) & = \mathrm{softmax}_k(a_k) \notag \\
  & =  \mathrm{softmax}_k( \vw_k^\top \vx + w_{k0} )
\end{align}
\begin{align}
  \vw_k & = \mSigma^{-1} \vmu_k \\
  w_{k0} & = -\frac{1}{2} \vmu_k^\top \mSigma^{-1} \vmu_k + \ln p(\cC_k)
\end{align}

\begin{itemize}
  \item $a_k$ が $\vx$ の線形関数となる．
  \item 誤分類率をロス関数とした場合（正解クラスに依らず，分類に成功したとき/失敗した時に課せられるペナルティが全て同じ場合）の境界面は，やはり $\vx$ の線形関数．
\end{itemize}

\setcounter{equation}{70}
\subsection*{4.2 最尤解}

クラスの条件付き確率 $p(\vx \given \cC_k)$ にパラメトリックな関数を定めると，クラスの事前確率 $p(\cC_k)$ とともに，パラメータの値を最尤法を用いて決めることができる．

\begin{alertinfo}
？？？「任意の入力 $\vx$ に対して，``クラス $\cC_k$ が $\vx$ を生成する確率''をいい感じに出力する関数を自分で設定すれば，その関数は最尤法っていう学習方法でデータに fit させることができるんやで」
\end{alertinfo}

\subsubsection*{2クラス}

$\vx_n \in \R^M,\, t_n \in \set{0,1}$ な訓練集合 $\cD \defeq \set{\vx_n, t_n}_{n=1}^N$ が与えられている．
\begin{itemize}
  \item $t_n = 0 \implies \vx_n$ はクラス $\cC_1$
  \item $t_n = 1 \implies \vx_n$ はクラス $\cC_2$
\end{itemize}

ここで，クラスの事前分布を $p(\cC_1) = \pi,\, p(\cC_2) = 1 - \pi$ とすると，尤度関数は
\begin{align}
  p(\cD \given \pi, \vmu_1, \vmu_2, \mSigma) & = \prod_n \braces{\pi p(\vx \given \cC_1)}^{t_n} \braces{(1 - \pi) p(\vx \given \cC_2)}^{1 - t_n} \notag \\
  & = \prod_n \braces{\pi \cN(\vx_n \given \vmu_1, \mSigma)}^{t_n} \braces{(1 - \pi) \cN(\vx_n \given \vmu_2, \mSigma)}^{1 - t_n}
\end{align}

\paragraph{$\pi$ を最尤推定する}

対数尤度 $\ln L(\pi, \vmu_1, \vmu_2, \mSigma)$ で $\pi$ が関係しているのは
\begin{equation}
   = \sum_n \braces{t_n \ln \pi + (1-t_n)\ln(1 - \pi)}
\end{equation}
なので，$\pp{\ln L}{\pi} = 0$ を解くと
\begin{equation}
  \pi = \frac{N_1}{N_1 + N_2}
\end{equation}
が得られる．ただし，$N_1,N_2$ はそれぞれ $\cC_1, \cC_2$ に属するデータの数．

\paragraph{$\vmu_1$ を最尤推定する}

$\vmu_1$ が関係しているのは
\begin{equation}
  \sum_n t_n \ln \cN(\vx_n \given \vmu_1, \mSigma) = - \frac{1}{2} \sum_n t_n (\vx_n - \vmu_1)^\top \mSigma^{-1} (\vx_n - \vmu_1) + \mathrm{const.}
\end{equation}
なので，$\pp{\ln L}{\vmu_1} = 0$ を解くと
\begin{equation}
  \vmu_1 = \frac{1}{N_1} \sum_n t_n \vx_n
\end{equation}

$\vmu_2$ についても同様に解ける．
\begin{equation}
  \vmu_2 = \frac{1}{N_2} \sum_n (1 - t_n) \vx_n
\end{equation}

というのも，